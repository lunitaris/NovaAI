import httpx
import json
import logging
from CoreIA.graph_memory import get_graph_memory

OLLAMA_API = "http://localhost:11434/api"
logger = logging.getLogger("chat-engine")

# Obtenir l'instance de la m√©moire graphe (singleton)
graph_memory = get_graph_memory()

async def prepare_conversation(user_message: str, history: list, session_id=None) -> list:
    """
    Pr√©pare le contexte de conversation en int√©grant la m√©moire graphe.
    
    :param user_message: Message de l'utilisateur
    :param history: Historique de conversation de la session actuelle
    :param session_id: Identifiant de session optionnel
    :return: Liste format√©e pour le LLM
    """
    # Utiliser directement la m√©thode de pr√©paration du contexte de la m√©moire graphe
    conversation = graph_memory.prepare_conversation_context(
        user_message, 
        session_id=session_id,
        k_similar=5,  # Nombre d'interactions similaires √† inclure
        k_summaries=3  # Nombre de r√©sum√©s th√©matiques √† inclure
    )
    
    # Logs pour le d√©bogage
    similar_memories = graph_memory.search_similar(user_message, k=5)
    print("üß† M√©moire s√©mantique retrouv√©e (Graph):")
    for i, item in enumerate(similar_memories):
        print(f"  #{i+1} ‚û§ {item['assistant'][:100]}... (score: {item['similarity']:.2f})")

    summaries = graph_memory.get_summaries(limit=3)
    print("üìÑ R√©sum√©s synth√©tiques inject√©s :")
    for i, entry in enumerate(summaries[:3]):
        print(f"  #{i+1} ‚û§ {entry['summary'][:100]}...")

    return conversation


async def get_llm_response(conversation: list, model: str = "llama3", stream: bool = False, session_id=None) -> dict:
    """
    Obtient une r√©ponse du LLM et met √† jour la m√©moire graphe.
    
    :param conversation: Liste format√©e pour le LLM
    :param model: Mod√®le Ollama √† utiliser
    :param stream: Mode de streaming
    :param session_id: Identifiant de session optionnel
    :return: R√©ponse du LLM
    """
    payload = {
        "model": model,
        "messages": conversation,
        "stream": stream,
        "options": {
            "temperature": 0.7,
            "num_predict": 256,
            "top_p": 0.95,
            "top_k": 30
        }
    }

    async with httpx.AsyncClient(timeout=60.0) as client:
        response = await client.post(f"{OLLAMA_API}/chat", json=payload)
        result = response.json()

    if not stream:
        try:
            # Extraire les messages pour la m√©moire
            user_message = conversation[-1]["content"]
            assistant_response = result.get("message", {}).get("content", "")
            
            # Ajouter √† la m√©moire graphe
            graph_memory.add_message("user", user_message, session_id=session_id)
            assistant_msg_id = graph_memory.add_message("assistant", assistant_response, session_id=session_id)
            
            # G√©n√©rer un r√©sum√© si n√©cessaire
            from CoreIA.summary_engine import SummaryEngine
            summary_engine = SummaryEngine()
            
            summary, importance = await summary_engine.summarize(user_message)
            if summary:
                theme = await summary_engine.extract_theme(summary)
                graph_memory.add_summary(
                    theme=theme, 
                    summary=summary, 
                    importance=importance,
                    related_messages=[assistant_msg_id]  # Lier le r√©sum√© au message
                )
                print(f"[MEMO GRAPH] R√©sum√© : {summary[:60]}... ‚Üí Th√®me : {theme}")

        except Exception as e:
            logger.error(f"[WARN] Failed to store memory: {e}")

    return result