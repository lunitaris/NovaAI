# Assistant IA Local

Un assistant IA conversationnel local avec interface web futuriste et reconnaissance vocale.

## Description

Ce projet impl√©mente un assistant IA enti√®rement local utilisant Ollama comme backend pour les mod√®les de langage, avec une interface web r√©active et une reconnaissance vocale bas√©e sur whisper.cpp. L'assistant r√©pond √† vos questions en mode texte ou vocal, avec une visualisation dynamique de son √©tat.

## Fonctionnalit√©s

- üí¨ Chat avec interface web r√©active
- üé§ Reconnaissance vocale bas√©e sur whisper.cpp
- üé≠ Visualisation dynamique de l'√©tat de l'IA avec animations
- üîÑ S√©lection et utilisation de diff√©rents mod√®les LLM (via Ollama)
- üéØ Optimis√© pour la rapidit√© et la r√©activit√©

## Pr√©requis

- Python 3.8+
- [Ollama](https://github.com/ollama/ollama) install√© et configur√©
- [whisper.cpp](https://github.com/ggerganov/whisper.cpp) compil√© localement

## Installation

1. Clonez ce d√©p√¥t :
```bash
git clone https://github.com/votre-nom/assistant-ia-local.git
cd assistant-ia-local
```

2. Cr√©ez un environnement virtuel et installez les d√©pendances :
```bash
python -m venv venv
source venv/bin/activate  # Sur macOS/Linux
pip install -r requirements.txt
```

3. Assurez-vous d'avoir whisper.cpp compil√© :
```bash
# Si vous n'avez pas encore whisper.cpp
git clone https://github.com/ggerganov/whisper.cpp.git opt/whisper.cpp
cd opt/whisper.cpp
make
# T√©l√©chargez un mod√®le (base ou medium recommand√©)
bash ./models/download-ggml-model.sh base
cd ../..
```

4. Assurez-vous d'avoir au moins un mod√®le t√©l√©charg√© dans Ollama :
```bash
ollama pull llama3
```

## Utilisation

1. Lancez le script de d√©marrage :
```bash
./run.sh
```

2. Acc√©dez √† l'interface web √† l'adresse : http://localhost:8000

3. Commencez √† discuter avec l'assistant :
   - Tapez votre message et appuyez sur Entr√©e
   - Ou cliquez sur l'ic√¥ne du microphone pour parler

## Structure du projet

- `app.py`: Application principale FastAPI
- `voice_service.py`: Service Flask pour la reconnaissance vocale
- `static/`: R√©pertoire contenant les fichiers frontend
  - `index.html`: Structure de l'interface
  - `styles.css`: Styles et animations
  - `script.js`: Logique frontend
- `run.sh`: Script de d√©marrage des services

## Configuration

- Les chemins vers whisper.cpp peuvent √™tre configur√©s dans `voice_service.py`
- Les param√®tres de reconnaissance vocale sont ajustables dans `voice_service.py`
- L'URL de l'API Ollama peut √™tre configur√©e dans `app.py`

## R√©solution des probl√®mes

- **Port occup√©**: Si le port 5000 ou 5001 est d√©j√† utilis√©, modifiez-le dans `voice_service.py` et `script.js`
- **Erreurs GPU**: Si vous rencontrez des erreurs GPU avec whisper.cpp, assurez-vous que l'option `--no-gpu` est activ√©e dans `voice_service.py`
- **Probl√®mes de transcription**: Essayez d'utiliser un mod√®le plus petit (tiny ou base) pour am√©liorer la vitesse

## Am√©liorations futures

- Ajout d'une base de connaissances personnelle
- Connecteurs pour services domotiques
- Synth√®se vocale pour les r√©ponses
- Acc√®s et analyse de fichiers locaux
- Migration vers un syst√®me d√©di√© (Raspberry Pi/NUC)

## Licence

MIT

---

## requirements.txt

```
fastapi==0.103.1
uvicorn==0.23.2
flask==2.3.3
flask-cors==4.0.0
sounddevice==0.4.6
numpy==1.24.4
wave==0.0.2
python-dotenv==1.0.0
pydantic==2.4.2
requests==2.31.0
```